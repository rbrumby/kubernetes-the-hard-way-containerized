# Starting etcd
When we created the kubelet configuration (/var/lib/kubelet/config.yaml), we set the staticPodPath to /etc/kubernetes/manifests.
Kubelet will monitor this directory for pod yaml files & will start up those pods & register them with the API server (once we have one).

***IMPORTANT***: If you are following along & building a 3-node control plane with a 3 node etcd cluster, you should complete the steps below on ***all nodes*** (at least a quorum of nodes is required for a successful election)

## Create the etcd yaml in the manifests directory
```
. /etc/kubernetes/env.sh

cat <<EOF | sudo tee /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://${CURRENT_NODE}:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    # Advertise client connections on the VIP address (.50 & .51 are both on the same NIC)
    - --listen-client-urls=https://0.0.0.0:2379
    - --advertise-client-urls=https://${CURRENT_NODE}:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --listen-peer-urls=https://0.0.0.0:2380
    - --initial-advertise-peer-urls=https://${CURRENT_NODE}:2380
    - --initial-cluster=${ETCD_MEMBERS}
    - --initial-cluster-state=new
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --name=$(echo ${CURRENT_NODE} | cut -d '.' -f1)
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: k8s.gcr.io/etcd:3.4.13-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
EOF
```
### What just happened?
- We created a pod manifest for etcd in the kubelets static pod manifests directory.
- We mounted the hosts pki/etcd directory into the container so that etcd can access the certificates.
- We also mounted a host directory for etcd to store its data. As this is a static pod, it will only ever run on this kubelet so using a host volume for data is fine.
- We set it up to advertise its client address on the .50 address (as that is the VIP/load balancer). Clients can connect to any node, we don't want them to depend on the first one being available.
- We also specified the other nodes in the cluster (that we plan to create later) with the peer URL set to this specific node as the peers will need to connect to each other individually.
- We don't have an API server yet so we can't use kubectl to look at what happened. Instead, we can use crictl, so if everything is right, you should be able to see etcd running:
```
ps -ef|grep etcd
sudo crictl ps
```
- If not, take a look at the logs using:
```
sudo crictl ps -a
sudo crictl logs <find-the-container-id-from-previous-command>
```
- You can also check kubelet logs with something like:
```
journalctl -u kubelet --since "10 minutes ago"
```
Note: If you want to use crictl without sudo, you can change the group ownership & permissions (using chown & chmod) of /run/containerd/containerd.sock to something suitable for your environment.
