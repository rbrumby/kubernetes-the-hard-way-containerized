# Configuring & starting the kube-controller-manager
The Controller Manager does many things. Currently, we are particularly interested in getting it to approve & issue a certificate against the kubelets certificate signing request.

## Create the kubeconfig file for the controller manager to use:
```
. /etc/kubernetes/env.sh

sudo kubectl config set-cluster k8sc01 \
  --embed-certs=false \
  --certificate-authority=/etc/kubernetes/pki/ca.crt \
  --server=https://${MASTER_LB}:6443 \
  --kubeconfig=/etc/kubernetes/controller-manager.conf

sudo kubectl config set-credentials system:kube-controller-manager \
  --embed-certs=false \
  --client-certificate=/etc/kubernetes/pki/controller-manager.crt \
  --client-key=/etc/kubernetes/pki/controller-manager.key \
  --kubeconfig=/etc/kubernetes/controller-manager.conf

sudo kubectl config set-context system:kube-controller-manager@k8sc01 \
  --cluster=k8sc01 \
  --user=system:kube-controller-manager \
  --kubeconfig=/etc/kubernetes/controller-manager.conf

sudo kubectl config use-context system:kube-controller-manager@k8sc01 \
  --kubeconfig=/etc/kubernetes/controller-manager.conf
```
### What just happened?
- Nothing particularly new or exciting here - we just created a kubeconfig file for the Controller Manager to use using the certificates we created for that purpose earlier.

## Create the controller-manager pod manifest:
(in the kubelets static pod manifest directory)
```
. /etc/kubernetes/env.sh

cat <<EOF | sudo tee /etc/kubernetes/manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --allocate-node-cidrs=true
    - --cluster-cidr=10.32.0.0/12
    - --service-cluster-ip-range=${SERVICE_CIDR}
    - --use-service-account-credentials=true
    image: k8s.gcr.io/kube-controller-manager:v1.20.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priorityClassName: system-node-critical
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
EOF
```

### What just happened?
- We configured the controller manager as a static pod, just like we did for etcd & the API Server.
- We mounted volumes to provide access to our certificates & the kubeconfig file we created above so that the Controller Manager can communicate with the API Server.
- We set cluster-cidr to 10.32.0.0/12 because this is the default range weave will use (makes installing weave simpler). You can change this if you want but you will have to check the [weave documentation](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install) for details.
- We should be able to see the Controller Manager running:
```
ps -ef|grep kube-controller-manager
sudo crictl ps
```
- We should now be able to see that the CSR that the kubelet submitted is approved & issued.
```
kubectl get csr
```
- As a result of this, the kubelet should have:
  - Obtained the certificate, stored it in the /var/lib/kubelet/pki directory & made a symbolic link to it:
  ```
  ls -l /var/lib/kubelet/pki
  ```
  - Created the "/etc/kubernetes/kubelet.conf" file containing references to the certificate above.
- The controller manager creates a whole heap of namespace default service accounts,  service account secrets, config maps, etc.
- The kubelet is now bootstrapped & should be visible as a node but will show in a NotReady status because we haven't yet configured networking:
```
kubectl get nodes
```
- We should also be able to see that the kubelet has registered its static pods with the API Server:
```
kubectl get po -n kube-system -owide
```
