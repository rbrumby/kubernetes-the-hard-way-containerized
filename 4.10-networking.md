# Setting up cluster networking
At this point, things are nearly there but the kubelet is still in a NotReady status.
If we describe nodes, NetworkNotReady is given as the reason & the logs in journalctl tell the same story.
There are 2 parts to setting up the network: kube-proxy & a pod network (CNI - we will use Weave).

Unlike the kubernetes components we deployed via the static pod manifests directory, kube-proxy needs to run on worker nodes.
We will therefore deploy it as a daemonset so that worker nodes that are added to the cluster will automatically run a kube-proxy pod on joining.
Because kube-proxy will run as a daemonset, we don't want to have to depend on certificates, keys, kubeconfigs & other files existing on each node.
Instead, on the host file system, it uses a configmap & serviceaccount token to make it more portable across the cluster.

## Create a serviceaccount for kube-proxy to use & give it a clusterrolebinding to system:node-proxier:
```
kubectl create serviceaccount -n kube-system kube-proxy

cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-proxy:node-proxier
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-proxier
subjects:
- kind: ServiceAccount
  name: kube-proxy
  namespace: kube-system
EOF
```
### What just happened?
- We created a service account that kube-proxy will use to authenticate with the API server.
- We created a cluster role binding associated with that service account which has the pre-defined "system:node-proxier" role which contains the necessary permissions.

## Create a ConfigMap for kube-proxy to use
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    app: kube-proxy
data:
  config.yaml: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    bindAddress: 0.0.0.0
    clientConnection:
      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  kubeconfig.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://${MASTER_LB}:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
EOF
```
### What just happened?
- We created a ConfigMap containing a KubeProxyConfiguration to configure kube-proxy pods when they are scheduled to nodes. That configuration references a kubeconfig which is also included in the ConfigMap & which uses our VIP address to access the API server. The kubeconfig uses the token of the service account we created above to authenticate.

## Create the kube-proxy daemonset:
```
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    k8s-app: kube-proxy
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    metadata:
      labels:
        k8s-app: kube-proxy
    spec:
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.yaml
        - --hostname-override=\$(NODE_NAME)
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        name: kube-proxy
        image: k8s.gcr.io/kube-proxy:v1.20.0
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kube-proxy
          name: kube-proxy
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      serviceAccount: kube-proxy
      serviceAccountName: kube-proxy
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - operator: Exists
      volumes:
      - configMap:
          defaultMode: 420
          name: kube-proxy
        name: kube-proxy
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - hostPath:
          path: /lib/modules
          type: ""
        name: lib-modules
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
EOF
```
### What just happened?
- We created the kube-proxy daemonset which will schedule a pod on every node in the cluster. Kube-proxy will maintain the networking rules on each node to enable traffic directed to a service to reach the backend pods that implement that service.
- We should now be able to see a kube-proxy pod running on our node:
```
kubectl get po -A
```
- It is worth checking the logs of that pod at this stage to make sure there aren't any issues.
```
kubectl logs -n kube-system kube-scheduler-<node-name>
```

## Install weave
```
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```
### What just happened?
- We installed the Weave CNI plugin which creates a daemonset to manage the pod network.
- Now that the network is ready, we should be able to see that our node status shows ready.
```
kubectl get no
```

## Set the master node taint
In a production environment, we wouldn't want to run pods on our master nodes - that's what the workers are for.
Adding the below taint stops the scheduler from scheduling pods which don't tolerate the taint from being scheduled on our master node.
If you are in a development environment with limited resources, you can skip adding the taint & your master node will also be schedulable & can run your workload pods in addition to the control plane components.
```
kubectl taint node k8sc01m01 node-role.kubernetes.io/master:NoSchedule
```
### What just happened?
- We set a taint on the master node to stop pods from being scheduled on the master node (unless they explicitly tolerate the taint).

## Install CoreDNS
The following is adapted from [here](https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.7.0.yaml) which has a hard-coded cluster IP for the DNS service which may not be compatible with our chosen service CIDR and also does not have a toleration for running on master nodes (the taint we just created above).
If you are happy using the default CIDR for services in the above linked yaml & you want CoreDNS to only run on worker nodes, you can "kubectl -f" the above URL directly instead of executing the below.
```
. /etc/kubernetes/env.sh

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        - key: node-role.kubernetes.io/master
          operator: "Exists"
          effect: NoSchedule
      nodeSelector:
        beta.kubernetes.io/os: linux
      containers:
      - name: coredns
        image: coredns/coredns:1.7.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: ${KUBEDNS_SVC_IP}
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
EOF
```
### What just happened?
- We installed CoreDNS which creates a deployment (with 2 replicas) to provide DNS for services in the cluster.


TODO: EncryptionConfig - see mumshad's repo
